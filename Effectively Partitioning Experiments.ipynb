{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "This notebook includes code for creating a json text file with the exact training sets and sample indeces used for training, sampling, and validation through the training, sampling, and validation pipeline used for this study.  The purpose is so that additional sampling or classification techniques can be used on independent runs without needing to run the pipeline from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resulting json file would like this :\n",
    "\n",
    "[{  \"minimum_personal_samples\" : 40, # minimum number of labels from user in order to be used in validation\n",
    "    \"k-folds\" : k, # k, for the k-fold cross-validation done on personal data\n",
    "    \"validation users\" : [\n",
    "                        {\"user_id\" : user_id,\n",
    "                         \"labeled personal samples\" : labeled_personal_samples, # all the samples available\n",
    "                         \"impersonal_training_set\" : {\"data_set_version\" : \"1\" # \"1\" if WISDM v1.1, \"2\" if WISDM 2.0\n",
    "                                                       \"data_set_technique\" : method_name # method used for selecting users or samples\n",
    "                                                       \"training users\" : [user_id1, user_id2, ...], # if set, \n",
    "                                                                                         #  the users included\n",
    "                                                                                         # in the training set\n",
    "                                                       \"training samples\" : [row_id1, row_id2, ...], # if set, \n",
    "                                                                                         # the row_ids included\n",
    "                                                       \"sample weights\" : [sw_1, sw_2,...]}, # if set, the sample weights\n",
    "                                           \"random_personal_samples\" : [[sample1, sample2, ...],[sample1, sample2, ...],...]\n",
    "                                           \"least_certain_personal_samples\" : [[sample1, sample2, ...],[sample1, sample2, ...],...]\n",
    "                                           \"margin_certain_personal_samples\" : {\"margin\":margin, # margin by which samples are chosen for labeling\n",
    "                                           samples\" : [[sample1, sample2, ...],[sample1, sample2, ...],...]\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wisdm import wisdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wisdm.set_data(version=\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_personal_sampling(personal_set, impersonal_set, number_of_samples):\n",
    "    random_active_indeces = np.random.choice(len(personal_set), ts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pipeline1(version, output_path, user_ids, training_set_techniques = [], k=10, minimum_personal_samples=40, make_compatible=True):\n",
    "\t# initialize pipeline variables\n",
    "\trandom_sample_iterations = 5\n",
    "\t\n",
    "\ttraining_sizes = [10,20,30,40,50,60,70,80,90,100]\n",
    "    \n",
    "    sampling_data = [{\"minimum_personal_samples\" : minimum_personal_samples,\n",
    "                      \"k-folds\" : k,\n",
    "                      \"validation_users\" : []}]\n",
    "    \n",
    "\twith warnings.catch_warnings():\n",
    "\t\twarnings.simplefilter(\"ignore\")\n",
    "\n",
    "\t\t# Train model with v1.1 data and get clusterings\n",
    "\t\tset_data(version=version, make_compatible=make_compatible)\n",
    "\n",
    "\t\tfor ind, user_id in enumerate(user_ids): # iterate through the users holding one out for testing\n",
    "\t\t\tuser_results = []\n",
    "\t\t\tprint(\"Running user #%s: %s\" % (ind, user_id))\n",
    "\t\t\tpersonal_set = get_user_set(user_id)\n",
    "\t\t\tpersonal_set = remove_all_nan(personal_set)\n",
    "\n",
    "\t\t\tprint(\"%s personal samples\" % len(personal_set))\n",
    "\n",
    "\t\t\tif len(personal_set) < minimum_personal_samples:\n",
    "\t\t\t\tprint(\"User %s has less than %s labeled samples...\" % (user_id, minimum_personal_samples))\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tpersonal_labels = np.array([t.decode(\"utf-8\") for t in personal_set['class'].as_matrix()])\n",
    "\t\t\tpersonal_features = personal_set.as_matrix(columns=[personal_set.columns[1:-1]])\n",
    "\n",
    "\t\t\t# get impersonal data\n",
    "\t\t\timpersonal_set = data_df[data_df['user'] != user_id]\n",
    "\t\t\timpersonal_set = remove_all_nan(impersonal_set)\n",
    "\t\t\timpersonal_labels = np.array([t.decode(\"utf-8\") for t in impersonal_set['class'].as_matrix()])\n",
    "\t\t\timpersonal_features = impersonal_set.as_matrix(columns=[impersonal_set.columns[1:-1]])\n",
    "            \n",
    "            validation_user_data = {\"user_id\" : user_id,\n",
    "                                    \"model training technique\" : []}\n",
    "\t\t\t\n",
    "            for training_set_technique in training_set_techniques:\n",
    "                \n",
    "            \n",
    "            \n",
    "            # train an impersonal model\n",
    "\t\t\timpersonal_scaler = StandardScaler().fit(impersonal_features)\n",
    "\t\t\tscaled_train_x = impersonal_scaler.transform(impersonal_features)\n",
    "\n",
    "\t\t\trfc_clf = weka_RF()\n",
    "\t\t\trfc_clf.fit(scaled_train_x, impersonal_labels)\n",
    "\n",
    "\t\t\t# calibrated for probability estimation\n",
    "\t\t\tprob_cal_cv_generator = StratifiedKFold(n_splits=3).split(impersonal_features,impersonal_labels)\n",
    "\t\t\tprob_cal_clf = CalibratedClassifierCV(rfc_clf, cv=prob_cal_cv_generator, method='sigmoid')\n",
    "\t\t\tprob_cal_clf.fit(scaled_train_x, impersonal_labels)\n",
    "\n",
    "\t\t\t# create clusters\n",
    "\t\t\tnumber_of_clusters = 4 # the higher this number is, the smaller we should expect each cluster to be\n",
    "\n",
    "\t\t\tKM = KMeans(n_clusters=number_of_clusters)\n",
    "\t\t\tclusters = KM.fit_predict(scaled_train_x)\n",
    "\n",
    "\t\t\t# split personal data into training (potentially) and test\n",
    "\t\t\tskf = StratifiedKFold(n_splits=k)\n",
    "\t\t\tk_run = 0\n",
    "\n",
    "\t\t\tfor active_index, test_index in skf.split(personal_features, personal_labels):\n",
    "\t\t\t\tprint(\"\\tRunning Fold #%s\\n\" % k_run)\n",
    "\t\t\t\t# data set available for active labeling from the individual\n",
    "\t\t\t\tall_active_features = personal_features[active_index]\n",
    "\t\t\t\tall_active_labels = personal_labels[active_index]\n",
    "\n",
    "\t\t\t\t# held out test set from individual\n",
    "\t\t\t\ttest_features = personal_features[test_index]\n",
    "\t\t\t\ttest_labels = personal_labels[test_index]\n",
    "\t\t\t\n",
    "\t\t\t\tk_run_df = sample_experiments(user_id, k_run,\n",
    "\t\t\t\t\t\t\t  impersonal_features, impersonal_labels, \\\n",
    "\t\t\t\t\t\t\t  all_active_features, all_active_labels, \\\n",
    "\t\t\t\t\t\t\t  test_features, test_labels, \\\n",
    "\t\t\t\t\t\t\t  training_sizes, \\\n",
    "\t\t\t\t\t\t\t  random_sample_iterations, \\\n",
    "\t\t\t\t\t\t\t  impersonal_model=prob_cal_clf, impersonal_scaler=impersonal_scaler,\n",
    "\t\t\t\t\t\t\t  KM=KM, clusters=clusters)\n",
    "\t\t\t\tuser_results.append(k_run_df)\n",
    "\t\t\t\tk_run += 1\n",
    "\t\t\tuser_scores_df = pd.concat(user_results)\n",
    "\t\t\tuser_scores_df.to_pickle(output_path+user_id+\".pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
