{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo :\n",
    "\n",
    "* ~~consolidate code from notebook into WISDM helper methods file~~\n",
    "* rewrite kfolds process so that we have more precise control over the size of a fold (as oppose to just the number of folds)\n",
    "* run experiment that directly compares model using ALL general data + active data to model using ONLY general data from nearest cluster + active data\n",
    "* analyze/visualize clusters (is there a better algorithm? is there a better k for the k-means?) \n",
    "* perhaps compare with using the WORST cluster, or using ONLY the personal data\n",
    "    * for each size of active data\n",
    "        * for each algorithm (also ensemble algorithm?)\n",
    "            * personal only\n",
    "            * universal only\n",
    "            * personal + ALL universal\n",
    "            * personal + best cluster universal\n",
    "            * personal + worst cluster universal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wisdm import wisdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personal Note : Consider adding wget commands here to download the WISDM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wisdm_1_dir = \"../../WISDM_ar_v1.1/\"\n",
    "wisdm_1_transformed_data  = \"WISDM_ar_v1.1_transformed_FIXED.arff\"\n",
    "wisdm_2_dir = \"../../WISDM_at_v2.0/\"\n",
    "wisdm_2_transformed_data = \"WISDM_at_v2.0_transformed_FIXED.arff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wisdm.WISDM_DIR = wisdm_1_dir\n",
    "wisdm.WISDM_TRANSFORMED = wisdm_1_transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../../WISDM_ar_v1.1/WISDM_ar_v1.1_transformed.arff\", \"r\") as fIn:\n",
    "    old_lines = fIn.readlines()\n",
    "    new_lines = []\n",
    "    for ind, line in enumerate(old_lines):\n",
    "        if ind is 3:\n",
    "            bracket_index = line.find(\"{\")\n",
    "            new_line = line[:bracket_index]\n",
    "            new_line += line[bracket_index:].replace('\"', '')\n",
    "        elif ind is 47:\n",
    "            bracket_index = line.find(\"{\")\n",
    "            new_line = line[:bracket_index]\n",
    "            new_line += \" \"+line[bracket_index:].replace('\"', '')\n",
    "        else:\n",
    "            new_line = line\n",
    "        new_lines.append(new_line)\n",
    "\n",
    "with open(\"../../WISDM_ar_v1.1/WISDM_ar_v1.1_transformed_FIXED.arff\", \"w\") as fOut:\n",
    "    for line in new_lines:\n",
    "        fOut.write(line)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FIXING WISDM DATA SO IT CAN LOAD with loadarff()\n",
    "with open(\"../../WISDM_at_v2.0/WISDM_at_v2.0_transformed.arff\", \"r\") as fIn:\n",
    "    old_lines = fIn.readlines()\n",
    "    new_lines = []\n",
    "    for ind, line in enumerate(old_lines):\n",
    "        if ind is 2:\n",
    "            brack_index = line.find(\"{\")\n",
    "            new_line = line[:bracket_index]\n",
    "            new_line += line[bracket_index:].replace('\"', '')\n",
    "        elif ind is 46:\n",
    "            bracket_index = line.find(\"{\")\n",
    "            new_line = line[:bracket_index]\n",
    "            new_line += \" \"+line[bracket_index:].replace('\"', '')\n",
    "        else:\n",
    "            new_line = line\n",
    "        new_lines.append(new_line)\n",
    "\n",
    "with open(\"../../WISDM_at_v2.0/WISDM_at_v2.0_transformed_FIXED.arff\", \"w\") as fOut:\n",
    "    for line in new_lines:\n",
    "        fOut.write(line)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wisdm.set_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(wisdm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Universal Impersonal Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for 33: 0.779\n",
      "Score for 17: 0.337\n",
      "Score for 20: 0.810\n",
      "Score for 29: 0.792\n",
      "Score for 13: 0.840\n",
      "Score for 15: 0.917\n",
      "Score for 6: 0.905\n",
      "Score for 27: 0.695\n",
      "Score for 36: 0.719\n",
      "Score for 18: 0.845\n",
      "Score for 32: 0.793\n",
      "Score for 35: 0.504\n",
      "Score for 11: 0.778\n",
      "Score for 16: 0.755\n",
      "Score for 5: 0.871\n",
      "Score for 10: 0.765\n",
      "Score for 28: 0.860\n",
      "Score for 26: 0.794\n",
      "Score for 14: 0.759\n",
      "Score for 24: 0.798\n",
      "Score for 12: 0.854\n",
      "Score for 23: 0.744\n",
      "Score for 4: 0.673\n",
      "Score for 30: 0.152\n",
      "Score for 34: 0.845\n",
      "Score for 8: 0.978\n",
      "Score for 31: 0.823\n",
      "Score for 21: 0.710\n",
      "Score for 3: 0.813\n",
      "Score for 22: 0.832\n",
      "Score for 1: 0.779\n",
      "Score for 9: 0.250\n",
      "Score for 25: 0.924\n",
      "Score for 2: 0.523\n",
      "Score for 7: 0.850\n",
      "Score for 19: 0.861\n",
      "RF results : M=0.74800, SD=0.17894\n"
     ]
    }
   ],
   "source": [
    "rf_results = []\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for user_id in wisdm.user_ids:\n",
    "        test_set = wisdm.get_user_set(user_id)\n",
    "        test_set = wisdm.remove_all_nan(test_set)\n",
    "\n",
    "        test_labels = np.array([t.decode(\"utf-8\") for t in test_set['class'].as_matrix()])\n",
    "        test_features = test_set.as_matrix(columns=[test_set.columns[1:-1]])\n",
    "\n",
    "        #print(\"%s labels, %s features\" % (len(test_labels), len(test_features)))\n",
    "        # training features & labels\n",
    "        training_set = wisdm.data_df[wisdm.data_df['user'] != user_id]\n",
    "        training_set = wisdm.remove_all_nan(training_set)\n",
    "        training_labels = np.array([t.decode(\"utf-8\") for t in training_set['class'].as_matrix()])\n",
    "        training_features = training_set.as_matrix(columns=[test_set.columns[1:-1]])\n",
    "\n",
    "        # normalize features\n",
    "        scaler = StandardScaler().fit(training_features)\n",
    "        scaled_train_x = scaler.transform(training_features)\n",
    "        scaled_test_x = scaler.transform(test_features)\n",
    "\n",
    "        clf = wisdm.weka_RF()\n",
    "        clf.fit(scaled_train_x, training_labels)\n",
    "        predictions = clf.predict(scaled_test_x)\n",
    "\n",
    "        score = accuracy_score(test_labels, predictions)\n",
    "        print(\"Score for %s: %.3f\" % (user_id, score))\n",
    "        rf_results.append(score)\n",
    "\n",
    "print(\"RF results : M=%.5f, SD=%.5f\" % (np.mean(rf_results), np.std(rf_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took about 19.46033477783203 seconds\n"
     ]
    }
   ],
   "source": [
    "finish = time.time()\n",
    "print(\"Took about %s seconds\" % (finish - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelized Universal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ipyparallel as ipp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = ipp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dview = c[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def universal_model():\n",
    "    rf_results = []\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        for user_id in user_ids:\n",
    "            test_set = wisdm.get_user_set(user_id)\n",
    "            test_set = wisdm.remove_all_nan(test_set)\n",
    "\n",
    "            test_labels = np.array([t.decode(\"utf-8\") for t in test_set['class'].as_matrix()])\n",
    "            test_features = test_set.as_matrix(columns=[test_set.columns[1:-1]])\n",
    "\n",
    "            #print(\"%s labels, %s features\" % (len(test_labels), len(test_features)))\n",
    "            # training features & labels\n",
    "            training_set = wisdm.data_df[wisdm.data_df['user'] != user_id]\n",
    "            training_set = wisdm.remove_all_nan(training_set)\n",
    "            training_labels = np.array([t.decode(\"utf-8\") for t in training_set['class'].as_matrix()])\n",
    "            training_features = training_set.as_matrix(columns=[test_set.columns[1:-1]])\n",
    "\n",
    "            # normalize features\n",
    "            scaler = StandardScaler().fit(training_features)\n",
    "            scaled_train_x = scaler.transform(training_features)\n",
    "            scaled_test_x = scaler.transform(test_features)\n",
    "\n",
    "            clf = wisdm.weka_RF()\n",
    "            clf.fit(scaled_train_x, training_labels)\n",
    "            predictions = clf.predict(scaled_test_x)\n",
    "\n",
    "            score = accuracy_score(test_labels, predictions)\n",
    "            print(\"Score for %s: %.3f\" % (user_id, score))\n",
    "            rf_results.append(score)\n",
    "\n",
    "    print(\"RF results : M=%.5f, SD=%.5f\" % (np.mean(rf_results), np.std(rf_results)))\n",
    "    return rf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scatter_result = dview.scatter(\"user_ids\", wisdm.user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['33', '17', '20'], dtype=object),\n",
       " array(['29', '13', '15'], dtype=object),\n",
       " array(['6', '27', '36'], dtype=object),\n",
       " array(['18', '32', '35'], dtype=object),\n",
       " array(['11', '16', '5'], dtype=object),\n",
       " array(['10', '28', '26'], dtype=object),\n",
       " array(['14', '24'], dtype=object),\n",
       " array(['12', '23'], dtype=object),\n",
       " array(['4', '30'], dtype=object),\n",
       " array(['34', '8'], dtype=object),\n",
       " array(['31', '21'], dtype=object),\n",
       " array(['3', '22'], dtype=object),\n",
       " array(['1', '9'], dtype=object),\n",
       " array(['25', '2'], dtype=object),\n",
       " array(['7', '19'], dtype=object)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dview['user_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_wisdm_helpers_path():\n",
    "    import sys\n",
    "    sys.path.append(\"/home/sac086/hcml_chapter/wisdm_model_personalization/notebooks/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AsyncResult: add_wisdm_helpers_path>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dview.apply_async(add_wisdm_helpers_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%px --local\n",
    "import warnings\n",
    "import os\n",
    "from wisdm import wisdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing warnings on engine(s)\n",
      "importing os on engine(s)\n",
      "importing wisdm from wisdm on engine(s)\n",
      "importing numpy on engine(s)\n",
      "importing pandas on engine(s)\n",
      "importing accuracy_score from sklearn.metrics on engine(s)\n",
      "importing StandardScaler from sklearn.preprocessing on engine(s)\n",
      "importing StratifiedKFold,StratifiedShuffleSplit from sklearn.model_selection on engine(s)\n",
      "importing KMeans from sklearn.cluster on engine(s)\n",
      "importing mode from scipy.stats on engine(s)\n"
     ]
    }
   ],
   "source": [
    "with dview.sync_imports():\n",
    "    import warnings\n",
    "    import os\n",
    "    from wisdm import wisdm\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "    from sklearn.cluster import KMeans\n",
    "    from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dview.block=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init():\n",
    "    wisdm_1_dir = \"/home/sac086/hcml_chapter/WISDM_ar_v1.1/\"\n",
    "    wisdm_1_transformed_data  = \"WISDM_ar_v1.1_transformed_FIXED.arff\"\n",
    "    wisdm_2_dir = \"/home/sac086/hcml_chapter/WISDM_at_v2.0/\"\n",
    "    wisdm_2_transformed_data = \"WISDM_at_v2.0_transformed_FIXED.arff\"\n",
    "    \n",
    "    wisdm.WISDM_DIR = wisdm_1_dir\n",
    "    wisdm.WISDM_TRANSFORMED = wisdm_1_transformed_data\n",
    "    \n",
    "    wisdm.set_data()\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dview.apply(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.74496644295302017, 0.40384615384615385, 0.78306878306878303],\n",
       " [0.7865168539325843, 0.84571428571428575, 0.91666666666666663],\n",
       " [0.91216216216216217, 0.67682926829268297, 0.69784172661870503],\n",
       " [0.85161290322580641, 0.7931034482758621, 0.5663716814159292],\n",
       " [0.76388888888888884, 0.80392156862745101, 0.86428571428571432],\n",
       " [0.76470588235294112, 0.90654205607476634, 0.79393939393939394],\n",
       " [0.69109947643979053, 0.79838709677419351],\n",
       " [0.84552845528455289, 0.73643410852713176],\n",
       " [0.71153846153846156, 0.15178571428571427],\n",
       " [0.8214285714285714, 0.97810218978102192],\n",
       " [0.83732057416267947, 0.62616822429906538],\n",
       " [0.8193548387096774, 0.84955752212389379],\n",
       " [0.76551724137931032, 0.203125],\n",
       " [0.9242424242424242, 0.54128440366972475],\n",
       " [0.82677165354330706, 0.86082474226804129]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dview.apply(universal_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 5.647056818008423 seconds\n"
     ]
    }
   ],
   "source": [
    "finish = time.time()\n",
    "print(\"Finished in %s seconds\" % (finish-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation with exact number of data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Attributes : \n",
    "* amount of training set from individual end user\n",
    "* amount of training set from impersonal data (other users)\n",
    "    * \"ALL\" all other data\n",
    "    * \"closest cluster\" only data from the closest cluster\n",
    "    * \"furthest cluster\" only data from the furthest cluster\n",
    "    * \"All - furthest cluster\" all other data EXCEPT data from furthest cluster\n",
    "* test user id\n",
    "* algorithm\n",
    "* algorithm parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%px --local\n",
    "def personal_model(active_features, active_labels, test_features, test_labels):\n",
    "    scaler = StandardScaler().fit(active_features)\n",
    "    scaled_train_x = scaler.transform(active_features)\n",
    "    scaled_test_x = scaler.transform(test_features)\n",
    "\n",
    "    rfc_clf = wisdm.weka_RF()\n",
    "    rfc_clf.fit(scaled_train_x, active_labels)\n",
    "    predictions = rfc_clf.predict(scaled_test_x)\n",
    "    score = accuracy_score(test_labels, predictions)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%px --local\n",
    "\n",
    "def universal_model(universal_features, universal_labels, test_features, test_labels):\n",
    "    scaler = StandardScaler().fit(universal_features)\n",
    "    scaled_train_x = scaler.transform(universal_features)\n",
    "    scaled_test_x = scaler.transform(test_features)\n",
    "\n",
    "    rfc_clf = wisdm.weka_RF()\n",
    "    rfc_clf.fit(scaled_train_x, universal_labels)\n",
    "    predictions = rfc_clf.predict(scaled_test_x)\n",
    "    score = accuracy_score(test_labels, predictions)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%px --local\n",
    "\n",
    "def universal_plus_personal_model(personal_features, personal_labels,\n",
    "                                  universal_features, universal_labels,\n",
    "                                  test_features, test_labels):\n",
    "    personal_plus_universal_features = np.vstack((personal_features, universal_features))\n",
    "    personal_plus_universal_labels = np.hstack((personal_labels, universal_labels))\n",
    "\n",
    "    scaler = StandardScaler().fit(personal_plus_universal_features)\n",
    "    scaled_train_x = scaler.transform(personal_plus_universal_features)\n",
    "    scaled_test_x = scaler.transform(test_features)\n",
    "\n",
    "    rfc_clf = wisdm.weka_RF()\n",
    "\n",
    "    rfc_clf.fit(scaled_train_x, personal_plus_universal_labels)\n",
    "    predictions = rfc_clf.predict(scaled_test_x)\n",
    "    score = accuracy_score(test_labels, predictions)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%px --local\n",
    "\n",
    "def cluster_plus_personal_model(personal_features, personal_labels,\n",
    "                                  universal_features, universal_labels,\n",
    "                                  test_features, test_labels, KM):\n",
    "    cluster_predictions = KM.predict(personal_features)\n",
    "    closest_cluster = mode(cluster_predictions).mode[0]\n",
    "\n",
    "    cluster_data_indeces = [i for i in range(len(clusters)) if clusters[i] == closest_cluster]\n",
    "    cluster_features = universal_features[cluster_data_indeces]\n",
    "    cluster_labels = universal_labels[cluster_data_indeces]\n",
    "\n",
    "    training_features = np.vstack((personal_features, cluster_features))\n",
    "    training_labels = np.hstack((personal_labels, cluster_labels))\n",
    "\n",
    "    scaler = StandardScaler().fit(training_features)\n",
    "    scaled_train_x = scaler.transform(training_features)\n",
    "    scaled_test_x = scaler.transform(test_features)\n",
    "\n",
    "    rfc_clf = wisdm.weka_RF()\n",
    "\n",
    "    rfc_clf.fit(scaled_train_x, training_labels)\n",
    "    predictions = rfc_clf.predict(scaled_test_x)\n",
    "    score = accuracy_score(test_labels, predictions)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%px --local\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_sizes = [10,20,30,40,50,60,70,80,90,100]\n",
    "\n",
    "def all_models():\n",
    "    scores = []\n",
    "    err = None\n",
    "    training_sizes = [10,20,30,40,50,60,70,80,90,100]\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        for ind, user_id in enumerate(user_ids):\n",
    "            user_scores_df = []\n",
    "            print(\"Running user #%s: %s\" % (ind, user_id))\n",
    "            personal_set = wisdm.get_user_set(user_id)\n",
    "            personal_set = wisdm.remove_all_nan(personal_set)\n",
    "\n",
    "            personal_labels = np.array([t.decode(\"utf-8\") for t in personal_set['class'].as_matrix()])\n",
    "            personal_features = personal_set.as_matrix(columns=[personal_set.columns[1:-1]])\n",
    "\n",
    "            # What is the distribution of labels for this participant?\n",
    "            personal_labels_distribution = Counter(personal_labels)\n",
    "            print(\"\\tHas %s labels : \" % len(personal_labels))\n",
    "            for label_key, number in personal_labels_distribution.items():\n",
    "                print(\"\\t\\t%s:%s\" % (label_key, number))\n",
    "            print(\"\\n\")\n",
    "            # training features & labels\n",
    "            universal_set = wisdm.data_df[wisdm.data_df['user'] != user_id]\n",
    "            universal_set = wisdm.remove_all_nan(universal_set)\n",
    "            universal_labels = np.array([t.decode(\"utf-8\") for t in universal_set['class'].as_matrix()])\n",
    "            universal_features = universal_set.as_matrix(columns=[universal_set.columns[1:-1]])\n",
    "\n",
    "            # get k-means clusters\n",
    "            number_of_clusters = 4 # the higher this number is, the smaller we should expect each cluster to be\n",
    "\n",
    "            KM = KMeans(n_clusters=number_of_clusters)\n",
    "            clusters = KM.fit_predict(universal_features)\n",
    "\n",
    "            if clusters:\n",
    "                return clusters\n",
    "            k = 10\n",
    "\n",
    "            skf = StratifiedKFold(n_splits=k)\n",
    "\n",
    "            k_run = 0\n",
    "            for active_index, test_index in skf.split(personal_features, personal_labels):\n",
    "                print(\"\\tRunning Fold #%s\\n\" % k_run)\n",
    "                # data set available for active labeling from the individual\n",
    "                all_active_features = personal_features[active_index]\n",
    "                all_active_labels = personal_labels[active_index]\n",
    "\n",
    "\n",
    "                # held out test set from individual\n",
    "                test_features = personal_features[test_index]\n",
    "                test_labels = personal_labels[test_index]\n",
    "\n",
    "                # iterate through size of training data\n",
    "                for training_size in training_sizes:\n",
    "                    # initialize score holders\n",
    "                    personal_model_scores = []\n",
    "                    universal_model_scores = []\n",
    "                    personal_plus_all_scores = []\n",
    "                    personal_plus_cluster_scores = []\n",
    "\n",
    "                    # run universal model\n",
    "                    universal_model_score = universal_model(universal_features, universal_labels,\n",
    "                                                            test_features, test_labels)\n",
    "                    universal_model_scores.append(universal_model_score)\n",
    "\n",
    "                    sss = StratifiedShuffleSplit(n_splits=5, train_size=training_size, test_size=6)\n",
    "\n",
    "                    splits = sss.split(all_active_features, all_active_labels)\n",
    "\n",
    "                    try:\n",
    "                        for split_num, split_tup in enumerate(splits):\n",
    "                            sampled_active_index, __ = split_tup\n",
    "                            sampled_active_features = personal_features[sampled_active_index]\n",
    "                            sampled_active_labels = personal_labels[sampled_active_index]\n",
    "\n",
    "                            # run personal model\n",
    "                            personal_score = personal_model(sampled_active_features, sampled_active_labels, test_features, test_labels)\n",
    "                            personal_model_scores.append(personal_score)\n",
    "\n",
    "                            # run personal + universal\n",
    "                            personal_plus_all_score = universal_plus_personal_model(sampled_active_features, sampled_active_labels,\n",
    "                                                                                    universal_features, universal_labels,\n",
    "                                                                                    test_features, test_labels)\n",
    "                            personal_plus_all_scores.append(personal_plus_all_score)\n",
    "\n",
    "                            # run personal + cluster\n",
    "                            personal_plus_cluster_score = cluster_plus_personal_model(sampled_active_features, sampled_active_labels,\n",
    "                                                                                    universal_features, universal_labels,\n",
    "                                                                                    test_features, test_labels, KM)\n",
    "                            personal_plus_cluster_scores.append(personal_plus_cluster_score)\n",
    "                    except ValueError as ve:\n",
    "                        print(\"Error with training size during split #%s\" % split_num)\n",
    "                        print(\"Message : %s\" % ve.args[0])\n",
    "                        err = ve\n",
    "                        if \"Reduce test_size and/or train_size\" in ve.args[0]:\n",
    "                            print(\"continuing...\")\n",
    "                            continue\n",
    "                        elif \"should be smaller than the number of samples\" in ve.args[0]:\n",
    "                            print(\"continuing...\")\n",
    "                            continue\n",
    "                        elif \"The least populated class in y has only 1 member\" in ve.args[0]:\n",
    "                            print(\"continuing...\")\n",
    "                            continue\n",
    "                        else:\n",
    "                            raise(ve)\n",
    "\n",
    "                    row = {\"test user\" : user_id,\n",
    "                           \"k-run\" : k_run,\n",
    "                       \"classifier\" : \"RF with Wiki Parameters\",\n",
    "                       \"personal training data\" : training_size,\n",
    "                       \"personal score Mean\" : np.mean(personal_model_scores),\n",
    "                       \"personal score STD\" : np.std(personal_model_scores),\n",
    "                       \"impersonal score Mean\" : np.mean(universal_model_scores),\n",
    "                       \"impersonal score STD\" : np.std(universal_model_scores),\n",
    "                       \"personal + impersonal score Mean\" : np.mean(personal_plus_all_scores),\n",
    "                       \"personal + impersonal score STD\" : np.std(personal_plus_all_scores),\n",
    "                       \"personal + cluster score Mean\" : np.mean(personal_plus_cluster_scores),\n",
    "                       \"personal + cluster score STD\" : np.std(personal_plus_cluster_scores)\n",
    "                       }\n",
    "                    print(\"\\tamount of personal data : %s row\" % training_size)\n",
    "                    print(\"\\tpersonal model score : M=%.3f, SD=%.3f\" % (row[\"personal score Mean\"], row[\"personal score STD\"]))\n",
    "                    print(\"\\tuniversal model score : M=%.3f, SD=%.3f\" % (row[\"impersonal score Mean\"], row[\"impersonal score STD\"]))\n",
    "                    print(\"\\tpersonal + ALL universal : M=%.3f, SD=%.3f\" % (row[\"personal + impersonal score Mean\"], row[\"personal + impersonal score STD\"]))\n",
    "                    print(\"\\tpersonal + CLUSTER universal : M=%.3f, SD=%.3f\" % (row[\"personal + cluster score Mean\"], row[\"personal + cluster score STD\"]))\n",
    "                    print(\"\\n\")\n",
    "                    scores.append(row)\n",
    "                    user_scores_df.append(row)\n",
    "                k_run += 1\n",
    "            user_scores_df = pd.DataFrame(user_scores_df)\n",
    "            user_scores_df.to_pickle(\"/home/sac086/hcml_chapter/wisdm_model_personalization/notebooks/results/experiment_08-17/\"+user_id+\".pickle\")\n",
    "\n",
    "    scores_df = pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "CompositeError",
     "evalue": "one or more exceptions from call to method: all_models\n[0:apply]: ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n[1:apply]: ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n[2:apply]: ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n[3:apply]: ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n.... 11 more exceptions ...",
     "output_type": "error",
     "traceback": [
      "[0:apply]: ",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m",
      "\u001b[0;32m<ipython-input-33-8175ac799cea>\u001b[0m in \u001b[0;36mall_models\u001b[0;34m()\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
      "",
      "[1:apply]: ",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m",
      "\u001b[0;32m<ipython-input-33-8175ac799cea>\u001b[0m in \u001b[0;36mall_models\u001b[0;34m()\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
      "",
      "[2:apply]: ",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m",
      "\u001b[0;32m<ipython-input-33-8175ac799cea>\u001b[0m in \u001b[0;36mall_models\u001b[0;34m()\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
      "",
      "[3:apply]: ",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m",
      "\u001b[0;32m<ipython-input-33-8175ac799cea>\u001b[0m in \u001b[0;36mall_models\u001b[0;34m()\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
      "",
      "... 11 more exceptions ..."
     ]
    }
   ],
   "source": [
    "results = dview.apply(all_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finish = time.time()\n",
    "print(\"Finished all models in %s seconds\" % (finish - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Paralellizing to run on WISDM_2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_sizes = [10,20,30,40,50,60,70,80,90,100]\n",
    "\n",
    "scores = []\n",
    "err = None\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for ind, user_id in enumerate(wisdm.user_ids):\n",
    "        user_scores_df = []\n",
    "        print(\"Running user #%s: %s\" % (ind, user_id))\n",
    "        personal_set = wisdm.get_user_set(user_id)\n",
    "        personal_set = wisdm.remove_all_nan(personal_set)\n",
    "\n",
    "        personal_labels = np.array([t.decode(\"utf-8\") for t in personal_set['class'].as_matrix()])\n",
    "        personal_features = personal_set.as_matrix(columns=[personal_set.columns[1:-1]])\n",
    "        \n",
    "        # What is the distribution of labels for this participant?\n",
    "        personal_labels_distribution = Counter(personal_labels)\n",
    "        print(\"\\tHas %s labels : \" % len(personal_labels))\n",
    "        for label_key, number in personal_labels_distribution.items():\n",
    "            print(\"\\t\\t%s:%s\" % (label_key, number))\n",
    "        print(\"\\n\")\n",
    "        # training features & labels\n",
    "        universal_set = wisdm.data_df[wisdm.data_df['user'] != user_id]\n",
    "        universal_set = wisdm.remove_all_nan(universal_set)\n",
    "        universal_labels = np.array([t.decode(\"utf-8\") for t in universal_set['class'].as_matrix()])\n",
    "        universal_features = universal_set.as_matrix(columns=[universal_set.columns[1:-1]])\n",
    "        \n",
    "        # get k-means clusters\n",
    "        number_of_clusters = 4 # the higher this number is, the smaller we should expect each cluster to be\n",
    "\n",
    "        KM = KMeans(n_clusters=number_of_clusters)\n",
    "        clusters = KM.fit_predict(universal_features)\n",
    "        \n",
    "        k = 10\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=k)\n",
    "        \n",
    "        k_run = 0\n",
    "        for active_index, test_index in skf.split(personal_features, personal_labels):\n",
    "            print(\"\\tRunning Fold #%s\\n\" % k_run)\n",
    "            # data set available for active labeling from the individual\n",
    "            all_active_features = personal_features[active_index]\n",
    "            all_active_labels = personal_labels[active_index]\n",
    "\n",
    "\n",
    "            # held out test set from individual\n",
    "            test_features = personal_features[test_index]\n",
    "            test_labels = personal_labels[test_index]\n",
    "\n",
    "            # iterate through size of training data\n",
    "            for training_size in training_sizes:\n",
    "                # initialize score holders\n",
    "                personal_model_scores = []\n",
    "                universal_model_scores = []\n",
    "                personal_plus_all_scores = []\n",
    "                personal_plus_cluster_scores = []\n",
    "\n",
    "                # run universal model\n",
    "                universal_model_score = universal_model(universal_features, universal_labels,\n",
    "                                                        test_features, test_labels)\n",
    "                universal_model_scores.append(universal_model_score)\n",
    "\n",
    "                sss = StratifiedShuffleSplit(n_splits=5, train_size=training_size, test_size=6)\n",
    "\n",
    "                splits = sss.split(all_active_features, all_active_labels)\n",
    "\n",
    "                try:\n",
    "                    for split_num, split_tup in enumerate(splits):\n",
    "                        sampled_active_index, __ = split_tup\n",
    "                        sampled_active_features = personal_features[sampled_active_index]\n",
    "                        sampled_active_labels = personal_labels[sampled_active_index]\n",
    "\n",
    "                        # run personal model\n",
    "                        personal_score = personal_model(sampled_active_features, sampled_active_labels, test_features, test_labels)\n",
    "                        personal_model_scores.append(personal_score)\n",
    "\n",
    "                        # run personal + universal\n",
    "                        personal_plus_all_score = universal_plus_personal_model(sampled_active_features, sampled_active_labels,\n",
    "                                                                                universal_features, universal_labels,\n",
    "                                                                                test_features, test_labels)\n",
    "                        personal_plus_all_scores.append(personal_plus_all_score)\n",
    "\n",
    "                        # run personal + cluster\n",
    "                        personal_plus_cluster_score = cluster_plus_personal_model(sampled_active_features, sampled_active_labels,\n",
    "                                                                                universal_features, universal_labels,\n",
    "                                                                                test_features, test_labels, KM)\n",
    "                        personal_plus_cluster_scores.append(personal_plus_cluster_score)\n",
    "                except ValueError as ve:\n",
    "                    print(\"Error with training size during split #%s\" % split_num)\n",
    "                    print(\"Message : %s\" % ve.args[0])\n",
    "                    err = ve\n",
    "                    if \"Reduce test_size and/or train_size\" in ve.args[0]:\n",
    "                        print(\"continuing...\")\n",
    "                        continue\n",
    "                    elif \"should be smaller than the number of samples\" in ve.args[0]:\n",
    "                        print(\"continuing...\")\n",
    "                        continue\n",
    "                    elif \"The least populated class in y has only 1 member\" in ve.args[0]:\n",
    "                        print(\"continuing...\")\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise(ve)\n",
    "\n",
    "                row = {\"test user\" : user_id,\n",
    "                       \"k-run\" : k_run,\n",
    "                   \"classifier\" : \"RF with Wiki Parameters\",\n",
    "                   \"personal training data\" : training_size,\n",
    "                   \"personal score Mean\" : np.mean(personal_model_scores),\n",
    "                   \"personal score STD\" : np.std(personal_model_scores),\n",
    "                   \"impersonal score Mean\" : np.mean(universal_model_scores),\n",
    "                   \"impersonal score STD\" : np.std(universal_model_scores),\n",
    "                   \"personal + impersonal score Mean\" : np.mean(personal_plus_all_scores),\n",
    "                   \"personal + impersonal score STD\" : np.std(personal_plus_all_scores),\n",
    "                   \"personal + cluster score Mean\" : np.mean(personal_plus_cluster_scores),\n",
    "                   \"personal + cluster score STD\" : np.std(personal_plus_cluster_scores)\n",
    "                   }\n",
    "                print(\"\\tamount of personal data : %s row\" % training_size)\n",
    "                print(\"\\tpersonal model score : M=%.3f, SD=%.3f\" % (row[\"personal score Mean\"], row[\"personal score STD\"]))\n",
    "                print(\"\\tuniversal model score : M=%.3f, SD=%.3f\" % (row[\"impersonal score Mean\"], row[\"impersonal score STD\"]))\n",
    "                print(\"\\tpersonal + ALL universal : M=%.3f, SD=%.3f\" % (row[\"personal + impersonal score Mean\"], row[\"personal + impersonal score STD\"]))\n",
    "                print(\"\\tpersonal + CLUSTER universal : M=%.3f, SD=%.3f\" % (row[\"personal + cluster score Mean\"], row[\"personal + cluster score STD\"]))\n",
    "                print(\"\\n\")\n",
    "                scores.append(row)\n",
    "                user_scores_df.append(row)\n",
    "            k_run += 1\n",
    "        user_scores_df = pd.DataFrame(user_scores_df)\n",
    "        user_scores_df.to_pickle(\"./results/experiment_08-17/\"+user_id+\".pickle\")\n",
    "\n",
    "scores_df = pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
