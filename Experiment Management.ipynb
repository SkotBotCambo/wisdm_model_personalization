{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ~~parallelize experimental pipeline to make experiment iterations go faster on jakku~~\n",
    "* experiment with different clustering approaches\n",
    "    * different values for K in KMeans\n",
    "    * with and without standardization of the training data for clustering\n",
    "        * [a la this post](https://datascience.stackexchange.com/questions/6715/is-it-necessary-to-standardize-your-data-before-clustering)\n",
    "    * DBScan or some other clustering approach\n",
    "    * stratify data across activity classes to match labeled data\n",
    "* experiment with uncertainty sampling instead of random sampling of personal data\n",
    "* investigate clusters\n",
    "    * How big are the clusters? Is it just a matter of size of the cluster?\n",
    "    * are the clusters fairly homogenous/heterogenous with respect to\n",
    "        * activity\n",
    "        * user data (is a user's data mostly within one cluster, or many?)\n",
    "* investigate why this method works great for some people, but not others\n",
    "* What does this mean for designing human-in-the-loop systems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods\n",
    "\n",
    "## Training Methods\n",
    "***TODO***\n",
    "\n",
    "### Impersonal and Personal Models\n",
    "***TODO***\n",
    "\n",
    "### Impersonal + Personal Models\n",
    "***TODO***\n",
    "\n",
    "### Cluster of Impersonal Data + Personal Model\n",
    "***TODO***\n",
    "\n",
    "## Validation Method\n",
    "***TODO***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Experiment 1\n",
    "\n",
    "Comparing Universal + Personal data with varying amounts of personal data to 1 cluster of universal + personal data\n",
    "\n",
    "### Results with v1.1 dataset : \n",
    "On average, the nearest cluster + personal data did about the same as the universal + personal data, but there were far fewer really bad scenarios especially with little labeled data. Also, for some individuals, this approach consistently proved to perform best.\n",
    "\n",
    "### Results with v2.0 dataset :\n",
    "nearest cluster + personal data did better than universal + personal data on average. Consistently served as improvement across many users.\n",
    "\n",
    "### Results trained with v1.1 applied to v2.0:\n",
    "Impersonal + Personal model performs nearly perfectly with every amount of personal data. Why is this?\n",
    "\n",
    "** We assume that the labels are noisier in v2.0 data for two reasons: 1. The data was collected in the wild where users are more likely to input the correct activity incorrectly. 2. Training on this dataset and testing on a held out sample from the same dataset yields a result that is not promising. *How can this model be doing so well with noisy labels? You would think that there would be some more error due simply to user mislabeling* **\n",
    "\n",
    "\n",
    "### Follow-up Question #1: \n",
    "*Q: What is different about the moments when this approach works and when it doesn't work?*\n",
    "\n",
    "### Follow-up Question #2: \n",
    "*Q1 : Can we determine early in the use of the AR technology whether the personal, universal + personal, or cluster + personal approach will work best?*\n",
    "* Reason about this question along with the Tong Yu, Yong Zhuang, Ole Mengshoel, Osman Yagan paper (Hybridizing Personal and Impersonal Machine Learning Models for Activity Recognition On Mobile Devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow-up Question #3: \n",
    "For both the experiment in which the model was trained on v2.0 data and the experiment in which the model was trained on v1.1 and test on v2.0 data, the personal model did best for a surprisingly non-negligible amount of people when the dataset is smallest. This is counter-intuitive given that there is very very little data for the model to leverage here. \n",
    "\n",
    "*Q1 : Why does this work so well in the second two experiments?*\n",
    "\n",
    "*Q2 : Why didn't this work well for the first experiment?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow-up Question #4:\n",
    "Q : How do the class distributions in v2.0 differ from v1.1 and could that be affecting the differences in model performance? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2\n",
    "Using an uncertainty sampling approach to intelligently select data from available personal training set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
